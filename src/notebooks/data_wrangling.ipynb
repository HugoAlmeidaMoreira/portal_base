{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import & Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import os\n",
    "\n",
    "# Define the range of years\n",
    "start_year = 2012\n",
    "end_year = 2024\n",
    "\n",
    "# Function to read a single Excel file\n",
    "def read_excel_file(year):\n",
    "    file_path = f\"data/raw/contratospub{year}.xlsx\"\n",
    "    return pd.read_excel(file_path)\n",
    "\n",
    "# Use ThreadPoolExecutor to read files in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Map the read_excel_file function to the range of years\n",
    "    df_list = list(executor.map(read_excel_file, range(start_year, end_year + 1)))\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "combined_df.head()\n",
    "\n",
    "\n",
    "# Save as pickle file\n",
    "pkl_output_path = \"data/processed/combined_data.pkl\"\n",
    "combined_df.to_pickle(pkl_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the combined DataFrame from the pickle file\n",
    "pkl_input_path = \"data/processed/combined_data.pkl\"\n",
    "\n",
    "# Check if the pickle file exists before loading\n",
    "if os.path.exists(pkl_input_path):\n",
    "    combined_df = pd.read_pickle(pkl_input_path)\n",
    "else:\n",
    "    print(f\"File not found: {pkl_input_path}\")\n",
    "\n",
    "# Drop columns\n",
    "combined_df = combined_df.drop(columns=['Observacoes', 'numAcordoQuadro', 'DescrAcordoQuadro', 'concorrentes', 'regime', 'fundamentacao', 'nAnuncio', 'TipoAnuncio', 'idINCM', 'ProcedimentoCentralizado', 'dataFechoContrato', 'linkPecasProc', 'CritMateriais', 'tipoFimContrato', 'justifNReducEscrContrato', 'dataPublicacao'])\n",
    "\n",
    "# Separar a coluna adjudicante usando a primeira ocorrência de \" - \"\n",
    "combined_df['nif_adjudicante'] = combined_df['adjudicante'].str.split(' - ', n=1, expand=True)[0]\n",
    "combined_df['nome_adjudicante'] = combined_df['adjudicante'].str.split(' - ', n=1, expand=True)[1]\n",
    "\n",
    "# Remover a coluna original\n",
    "combined_df.drop(columns=['adjudicante'], inplace=True)\n",
    "\n",
    "# Separar a coluna cpv em código e descrição\n",
    "combined_df[['cpv_código', 'cpv_descricao']] = combined_df['cpv'].str.split(' - ', n=1, expand=True)\n",
    "\n",
    "# Criar a coluna cpv_division a partir dos dois primeiros números do cpv_código\n",
    "combined_df['cpv_division'] = combined_df['cpv_código'].str[:2]\n",
    "\n",
    "# Remover a coluna original\n",
    "combined_df.drop(columns=['cpv'], inplace=True)\n",
    "\n",
    "# Função para processar a geografia\n",
    "def process_geografia(entry):\n",
    "    if not isinstance(entry, str):\n",
    "        return 'Desconhecido', 'Desconhecido'\n",
    "    # Separar múltiplos locais pelo separador \"|\"\n",
    "    locais = entry.split(' | ')\n",
    "    # Extrair apenas NUTS I e NUTS II\n",
    "    nuts_ii = set()\n",
    "    for local in locais:\n",
    "        niveis = local.split(', ')\n",
    "        if len(niveis) >= 2:\n",
    "            nuts_ii.add(niveis[1])  # Pegamos no NUTS II\n",
    "        else:\n",
    "            nuts_ii.add('Portugal')  # Apenas Portugal (NUTS I)\n",
    "    \n",
    "    # Decidir o âmbito_geo\n",
    "    if len(nuts_ii) == 1:\n",
    "        if 'Portugal' in nuts_ii:\n",
    "            return 'Portugal', 'Nacional'\n",
    "        else:\n",
    "            return list(nuts_ii)[0], list(nuts_ii)[0]\n",
    "    else:\n",
    "        return list(nuts_ii)[0], 'Múltiplos'\n",
    "\n",
    "# Adicionar barra de progresso com tqdm\n",
    "tqdm.pandas(desc=\"Processando geografia\")\n",
    "\n",
    "# Aplicar a função à coluna geografia com tqdm\n",
    "combined_df[['NUTS', 'ambito_geo']] = combined_df['localExecucao'].progress_apply(process_geografia).apply(pd.Series)\n",
    "\n",
    "# Resultado final\n",
    "print(combined_df.head())\n",
    "\n",
    "# Save as pickle file\n",
    "pkl_output_path = \"data/processed/portal_base_trimmed.pkl\"\n",
    "combined_df.to_pickle(pkl_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine datasets portal & sioe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sioe_df = pd.read_excel('/workspaces/portal_base/data/raw/ExportResultadosPesquisa20241219T113545438Z.xlsx')\n",
    "\n",
    "# Drop columns: 'Contacto 4 -  Tipo', 'Contacto 4 -  Contacto' and 68 other columns\n",
    "sioe_df = sioe_df.drop(columns=['Morada 1 -  Latitude', ' NISS',' Código de Certidão Permanente de Registo','Morada 1 -  Longitude', 'Morada 1 -  Altitude', 'Contacto 4 -  Tipo', 'Contacto 4 -  Contacto', 'Contacto 4 -  Utilização', 'Contacto 4 -  Principal', 'Contacto 5 -  Tipo', 'Contacto 5 -  Contacto', 'Contacto 5 -  Utilização', 'Contacto 5 -  Principal', 'Contacto 6 -  Tipo', 'Contacto 6 -  Contacto', 'Contacto 6 -  Utilização', 'Contacto 6 -  Principal', 'Contacto 7 -  Tipo', 'Contacto 7 -  Contacto', 'Contacto 7 -  Utilização', 'Contacto 7 -  Principal', 'Contacto - Link 2 -  URL', 'Contacto - Link 2 -  Descrição', 'Contacto - Link 2 -  Classificação', 'Contacto - Link 3 -  URL', 'Contacto - Link 3 -  Descrição', 'Contacto - Link 3 -  Classificação', 'Contacto - Link 4 -  URL', 'Contacto - Link 4 -  Descrição', 'Contacto - Link 4 -  Classificação', 'Contacto - Link 5 -  URL', 'Contacto - Link 5 -  Descrição', 'Contacto - Link 5 -  Classificação', 'Contacto - Link 6 -  URL', 'Contacto - Link 6 -  Descrição', 'Contacto - Link 6 -  Classificação', 'Contacto - Link 7 -  URL', 'Contacto - Link 7 -  Descrição', 'Contacto - Link 7 -  Classificação', 'Contacto - Link 8 -  URL', 'Contacto - Link 8 -  Descrição', 'Contacto - Link 8 -  Classificação', 'Contacto - Link 9 -  URL', 'Contacto - Link 9 -  Descrição', 'Contacto - Link 9 -  Classificação', 'Contacto - Link 10 -  URL', 'Contacto - Link 10 -  Descrição', 'Contacto - Link 10 -  Classificação', 'Contacto - Link 11 -  URL', 'Contacto - Link 11 -  Descrição', 'Contacto - Link 11 -  Classificação', 'Contacto - Link 12 -  URL', 'Contacto - Link 12 -  Descrição', 'Contacto - Link 12 -  Classificação', 'Contacto - Link 13 -  URL', 'Contacto - Link 13 -  Descrição', 'Contacto - Link 13 -  Classificação', 'Contacto - Link 14 -  URL', 'Contacto - Link 14 -  Descrição', 'Contacto - Link 14 -  Classificação', 'Contacto - Link 15 -  URL', 'Contacto - Link 15 -  Descrição', 'Contacto - Link 15 -  Classificação', 'Contacto - Link 16 -  URL', 'Contacto - Link 16 -  Descrição', 'Contacto - Link 16 -  Classificação', 'Contacto - Link 17 -  URL', 'Contacto - Link 17 -  Descrição', 'Contacto - Link 17 -  Classificação', 'Contacto - Link 18 -  URL', 'Contacto - Link 18 -  Descrição', 'Contacto - Link 18 -  Classificação','Contacto - Link 19 -  URL', 'Contacto - Link 19 -  Descrição', 'Contacto - Link 19 -  Classificação'])\n",
    "\n",
    "\n",
    "# Separar a coluna adjudicante usando a primeira ocorrência de \" - \"\n",
    "sioe_df['cae_codigo'] = sioe_df[' CAE'].str.split(' - ', n=1, expand=True)[0]\n",
    "sioe_df['cae_atividade'] = sioe_df[' CAE'].str.split(' - ', n=1, expand=True)[1]\n",
    "sioe_df.drop(columns=[' CAE'], inplace=True)\n",
    "\n",
    "# Remover o sufixo '.0'\n",
    "sioe_df[' NIPC'] = sioe_df[' NIPC'].astype(str)\n",
    "sioe_df[' NIPC'] = sioe_df[' NIPC'].str.replace('.0', '', regex=False)\n",
    "\n",
    "#salvar como pickle \n",
    "sioe_df.to_pickle(\"/workspaces/portal_base/data/processed/sioe_base.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "portal_base_df = pd.read_pickle(\"/workspaces/portal_base/data/processed/portal_base_trimmed.pkl\")\n",
    "\n",
    "# import \"/workspaces/portal_base/data/processed/sioe_base.pkl\"\n",
    "sioe_df = pd.read_pickle(\"/workspaces/portal_base/data/processed/sioe_base.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar a junção (merge)\n",
    "merged_df = pd.merge(\n",
    "    portal_base_df,\n",
    "    sioe_df,\n",
    "    left_on=['nif_adjudicante', 'nome_adjudicante'],\n",
    "    right_on=[' NIPC', ' Designação'],\n",
    "    how='left',  # 'left' garante que mantemos todas as linhas de portal_base_df\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "# Filtrar as linhas de portal_base_df sem correspondência\n",
    "no_match_portal = merged_df[merged_df['_merge'] == 'left_only']\n",
    "\n",
    "# Contar as linhas sem correspondência\n",
    "num_no_match_portal = no_match_portal.shape[0]\n",
    "\n",
    "print(f\"Número de linhas de portal_base_df sem correspondência: {num_no_match_portal}\")\n",
    "\n",
    "# Verificar correspondência para 'nif_adjudicante' apenas\n",
    "nif_merged_df = pd.merge(\n",
    "    portal_base_df,\n",
    "    sioe_df,\n",
    "    left_on='nif_adjudicante',\n",
    "    right_on=' NIPC',\n",
    "    how='left',\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "nif_no_match = nif_merged_df[nif_merged_df['_merge'] == 'left_only']\n",
    "num_no_match_nif = nif_no_match.shape[0]\n",
    "\n",
    "print(f\"Número de linhas de portal_base_df sem correspondência em 'nif_adjudicante': {num_no_match_nif}\")\n",
    "\n",
    "# Verificar correspondência para 'nome_adjudicante' apenas\n",
    "nome_merged_df = pd.merge(\n",
    "    portal_base_df,\n",
    "    sioe_df,\n",
    "    left_on='nome_adjudicante',\n",
    "    right_on=' Designação',\n",
    "    how='left',\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "nome_no_match = nome_merged_df[nome_merged_df['_merge'] == 'left_only']\n",
    "num_no_match_nome = nome_no_match.shape[0]\n",
    "\n",
    "print(f\"Número de linhas de portal_base_df sem correspondência em 'nome_adjudicante': {num_no_match_nome}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar a junção para verificar correspondência apenas no 'nif_adjudicante'\n",
    "nif_merged_df = pd.merge(\n",
    "    portal_base_df,\n",
    "    sioe_df,\n",
    "    left_on='nif_adjudicante',\n",
    "    right_on=' NIPC',\n",
    "    how='left',\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "# Filtrar as linhas de portal_base_df sem correspondência no 'nif_adjudicante'\n",
    "nif_no_match = nif_merged_df[nif_merged_df['_merge'] == 'left_only']\n",
    "\n",
    "# Criar o subset apenas com as colunas de portal_base_df originais\n",
    "subset_no_match_nif = nif_no_match[portal_base_df.columns]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
